---
title: '文件资源管理平台详设'
date: 2023-1-5 15:07:29
cover: false
typora-root-url: 文件资源管理平台详设
---

# 背景

基于资源文件索引，实现文件管理

现状：以语料为例，不同语料的标注规则各有不同，语料的格式不一（excel、doc、pdf、json、txt等），存储的硬件环境不一（本地、外网的不同服务器上、内网的不同服务器上、制品库等）

需求：算法需要某一份标注数据，需要经历如下步骤：回忆在哪=>查找手工记录的归档文件（如果有）=>登录服务器（包含切换网络）=>进入资源目录

上面的流程有一点麻烦，但并不是大问题，重要的不能保证找到的那一份数据文件，是历史上所有的数据文件，也有可能在另外服务器的某个角落，有着类似的数据文件。并且即使归档文件记录了全量的数据，但文件实际分布在不同的环境，取文件的时候也是个小麻烦。如果能**全量提供**需求的数据，并在一个地方**统一取文件**就好了。

# 功能模块

## 技术选型

前台：

- Vue3

后台：

- Django

## 规划

能展示服务器下的资源目录（第三方标注工具的目录不在一起），这里的优化是统一到一个目录下，搭个文件服务器，界面提供下载

业务上时每个task对应一批（多个文件夹或多个文件）数据文件，可以给每个task打标签，通过标签以及分类来达到归档查询的目的，标签要带上是否加密的标签，未加密数据文件支持在线预览

实际业务多以要素标注为主，集成标注的每个要素：时间、地点、公司名等

涉及整理收集的文件，提供收集背景、下载及对应的说明即可

每个task提供原始数据文件、结果文件、工作区如果是第三方标注工具，应提供对应的第三方位置：如brat提供data的资源目录、labelstudio提供前台登录账号及projectID等；

支持关键字搜索，搜索的区域是每个task的标签及分类、展示的时候默认以时间降序





目前数据源是通过人工手动维护JSON文件，前台通过读取配置文件，进行展示

后续时间充裕的情况下，会基于Node写一个后台管理的界面，数据存在mysql中，并基于docker部署



## 必要的人工

平时只需要维护资源索引目录即可

定期如每月，上传至指定目录后，更新依赖的JSON配置文件（后期如果新增了后台管理），这个配置文件也可以复用，通过后台新增的数据，也同步到配置里



# 其他



语料管理系统

1.支持文件夹分类，或嵌套分类，以便区分日单位标注任务下的：原始语料、结果语料、说明文档等文件夹

至少是三栏布局，点击日任务时，右侧显示日任务的描述、关联关务、标注需求背景等字段

导出时，支持勾选，支持不同日，不同任务等细粒度勾选

预览应设置文件上线，过大的文件只能下载

文件夹中小文件语料过多时，要按需加载

任务指派时语料的处理，

- 原始语料需要放在后台指定路径（按需求分）





目前只做语料管理系统，但要考虑到后期对接语料标注系统



语料标注系统

- 要素标注
- 分类标注
- 校验
- 第三方



# 常见预料格式处理需求

pdf分页成pdf

pdf分页成图片

图片转pdf

支持多文件夹处理



常用工具xnConvert



# 文件备份自动化

设计出一套文件管理规则后，基于json文件对每个标注任务进行管理，及时记录各个信息。

并使用脚本进行备份

下面两个json文件，可以拆分成四个左右的表，暂时不做可视化操作维护。

文件格式规则

```json
[
    {
        "id": "0006",
        "project": "0006_苏粮合同单据_风控",
        "file": [
            {
                "filename": "",
                "file_origin": "",
                "get_file_time": ""
            },
            {
                "filename": "",
                "file_origin": "",
                "get_file_time": ""
            }
        ],
        "tag": [
            {
                "batch": "01",
                "tag_filename": "",
                "desc": "对苏粮合同中的各类pdf单据，进行切分，然后按类型汇总",
                "start_time": "",
                "end_time": "",
                "file_type": "",
                "file_number": "",
                "tools": "",
                "tools_path": "",
                "category": "03_数据整理",
                "external_data": [],
                "sovled": "1"
            },
            {
                "batch": "02",
                "tag_filename": "",
                "desc": "整理苏粮合同中的样例数据",
                "start_time": "",
                "end_time": "",
                "file_type": "",
                "file_number": "",
                "tools": "",
                "tools_path": "",
                "category": "03_数据整理",
                "external_data": [],
                "sovled": "1"
            }
        ]
    },
    {
        "id": "0007",
        "project": "0007_聊城审计档案_治理",
        "file": [
            {
                "filename": "0007_2022审计档案.rar",
                "file_origin": "知语群：聊城审计档案",
                "get_file_time": "2023-05-18"
            }
        ],
        "tag": [
            {
                "batch": "01",
                "tag_filename": "",
                "desc": "聊城审计的档案扫描样例数据已上传群共享，解压密码：8321055。请三位评估下：（1）客户的扫描文件我们能否处理，需不需要对客户的扫描做约束要求（2）客户提供的excel统计表格是否满足要求，是否需要调整 （3）对照南京市局，分析下聊城审计档案的数据跟南京的有多大差异，我们落地聊城审计档案需不需要额外的处理工作量",
                "start_time": "",
                "end_time": "",
                "file_type": "",
                "file_number": "",
                "tools": "",
                "tools_path": "",
                "category": "",
                "external_data": [],
                "sovled": "0"
            }
        ]
    },
    {
        "id": "0008",
        "project": "0008_海门检察院",
        "file": [
            {
                "filename": "0008_06海门检察院",
                "file_origin": "74/home/rhino/zkq/2023/06海门检察院/检察院土地执行",
                "get_file_time": "2023-05-19"
            }
        ],
        "tag": [
            {
                "batch": "01",
                "tag_filename": "0008_06海门检察院",
                "desc": "算法人员对文件夹中的行政决定处罚书pdf做了与处理后，给到txt，标注26号要求演示的8个要素",
                "start_time": "2023-05-22",
                "end_time": "2023-05-22",
                "file_type": "txt",
                "file_number": "238",
                "tools": "labelstudio",
                "tools_path": "http://10.45.150.74:8082/projects/76/data?tab=157",
                "category": "01_数据标注_NER",
                "external_data": [],
                "sovled": "1"
            }
        ]
    }
]
```

每个task下的json

```json
[
    {
        "batch": "01",
        "source_file_path": "0008_海门检察院/batch_01_演示数据标注/tag_origin_v1/*.txt",
        "tag_result_path": "0008_海门检察院/batch_01_演示数据标注/tag_result_v1/project-76-at-2023-05-22-16-00-f4e5a0fd.json",
        "backup_name": "0008_海门检察院/batch_01_演示数据标注/0008_backup_batch_01.zip"
    }
]
```

`backup.py`

```python
import os
import json
import glob
import shutil
import asyncio
ods_path = './ods/readme.json'
dwd_path = './dwd/'
backup_path = './backup/'

async def merge_readme():
    # ods目录下的readme.json
    ods_readme_content = []
    # 每个实际任务下的readme.json
    paths = glob.glob(dwd_path + '/*/readme.json')

    with open(ods_path, 'r', encoding='utf-8') as fr:
        ods_readme_content = json.load(fr)
    for task in ods_readme_content:
        id = task['id']
        for batch in task['tag']:
            external_data = batch['external_data']
            outer_batch_id = int(batch['batch'])
            for path in paths:
                if id in path:
                    with open(path, 'r', encoding='utf-8') as fr1:
                        task_content = json.load(fr1)
                        for task in task_content:
                            inner_batch_id = int(task['batch'])
                            if inner_batch_id == outer_batch_id:
                                external_data.append(task)
    with open('readme_merge.json', 'w') as fw:
        json.dump(ods_readme_content, fw, ensure_ascii=False)

async def backup():
    # 等待文件合并完成
    await merge_readme()

    zip_path_list = []
    rel_path = []
    with open('./readme_merge.json', 'r', encoding='utf-8') as fr:
        content = json.load(fr)
        for task in content:
            for item in task['tag']:
                backup_name = item['external_data'][0]['backup_name']
                if (backup_name != ''):
                    # 只带.zip后缀的文件名列表
                    zip_path_list.append(backup_name.split('/')[-1])
                    # 待备份的文件路径列表
                    rel_path.append(os.path.join('./dwd', backup_name))
    print(zip_path_list, rel_path)
    # 备份文件夹下的zip包列表
    backup_file_list = os.listdir('/app/tag/backup')
    # 两个index是同步对应的
    for i in range(len(zip_path_list)):
        # 检查是否已备份
        if zip_path_list[i] in backup_file_list:
            print('{0} 已备份过，跳过备份!'.format(zip_path_list[i]))
        else:
            print('正在备份{0}...'.format(zip_path_list[i]))
            shutil.copy(rel_path[i], backup_path)
            print('备份完成')

if __name__ == '__main__':
    asyncio.run(backup())
```

